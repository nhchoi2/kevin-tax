import streamlit as st
from huggingface_hub import InferenceClient
from dotenv import load_dotenv
import os
from pinecone import Pinecone, ServerlessSpec

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (.env íŒŒì¼)
load_dotenv()
api_key = os.getenv("HUGGINGFACE_API_KEY")
pinecone_api_key = os.getenv("PINECONE_API_KEY")
pinecone_env = os.getenv("PINECONE_ENV")
pinecone_index_name = os.getenv("PINECONE_INDEX_NAME")
pinecone_cloud = os.getenv("PINECONE_CLOUD")  # ì˜ˆ: 'aws'

# 2. Hugging Face API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
# - ì±„íŒ… ì‘ë‹µìš©: google/gemma-2-9b-it ëª¨ë¸
client = InferenceClient(provider="hf-inference", api_key=api_key)
# - ì„ë² ë”© ìƒì„±ìš©: sentence-transformers/all-MiniLM-L6-v2 ëª¨ë¸
embedding_client = InferenceClient(model="sentence-transformers/all-MiniLM-L6-v2", api_key=api_key)

# 3. Pinecone ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ìµœì‹  ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©)
pc = Pinecone(api_key=pinecone_api_key)
# Pinecone ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³ , ì—†ìœ¼ë©´ ìƒì„± (all-MiniLM-L6-v2 ì„ë² ë”© ì°¨ì›ì€ 384)
if pinecone_index_name not in [idx.name for idx in pc.list_indexes()]:
    pc.create_index(
        name=pinecone_index_name,
        dimension=384,
        metric='cosine',
        spec=ServerlessSpec(cloud=pinecone_cloud, region=pinecone_env)
    )
# ì¸ë±ìŠ¤ ì—°ê²°
index = pc.Index(pinecone_index_name)

# 4. Streamlit UI ì„¤ì •
st.set_page_config(page_title="ì„¸ë²•ì „ë¬¸ AI, Kevin", page_icon="ğŸ’¬", layout="wide")

# ì‚¬ì´ë“œë°”: ì„¤ì • ë° ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
with st.sidebar:
    st.header("ğŸ“Œ ì„¤ì •")
    clear_chat = st.button("ğŸ’¬ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”")
    if clear_chat:
        st.session_state.chat_history = []
        st.success("ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ë©”ì¸ íƒ€ì´í‹€ ë° ì„¤ëª…
st.title("ğŸ’¬ ì„¸ë²•ì „ë¬¸ AI, Kevin")
st.write("ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ Kevinì´ ë‹µë³€í•´ë“œë¦½ë‹ˆë‹¤.")

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

def get_embedding(text):
    result = embedding_client.feature_extraction(text)
    # ë§Œì•½ ê²°ê³¼ê°€ numpy ndarrayë¼ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    if hasattr(result, "tolist"):
        result = result.tolist()
    # ë§Œì•½ ë‹¤ì¤‘ í† í° ì„ë² ë”© ë¦¬ìŠ¤íŠ¸ë¼ë©´, ì²« ë²ˆì§¸ í† í°ì˜ ì„ë² ë”©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    if isinstance(result, list) and isinstance(result[0], list):
        return result[0]
    return result

def query_pinecone(query_text, top_k=3):
    """
    ì…ë ¥ ì§ˆë¬¸ì˜ ì„ë² ë”©ì„ ìƒì„±í•œ í›„, Pinecone ì¸ë±ìŠ¤ì—ì„œ ìœ ì‚¬ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    """
    embedding = get_embedding(query_text)
    if embedding is None:
        return None
    results = index.query(vector=embedding, top_k=top_k, include_metadata=True)
    return results

def generate_prompt(user_input, context):
    system_prompt = """
        ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ì„¸ë¬´ì‚¬ ê´€ë¦¬ìì…ë‹ˆë‹¤. í•­ìƒ ì¹œì ˆí•˜ê³  ìì„¸í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
        ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´, í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•´ ì „ë¬¸ì ì¸ ì„¸ë¬´ ìƒë‹´ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.

        - ì§ˆë¬¸ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ êµ¬ì²´ì ì¸ ë‚´ìš©ì´ ë¶€ì¡±í•˜ë©´ ë°˜ë“œì‹œ "ì§ˆë¬¸ì— ëŒ€í•œ ë‚´ìš©ì´ ë§ì•„ ë‹µë³€ì´ ì–´ë µìŠµë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”"ë¼ëŠ” ë©”ì‹œì§€ë¡œ ì‘ë‹µí•˜ì‹­ì‹œì˜¤.
        - Pineconeì—ì„œ ì ì ˆí•œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°, ì¼ë§ˆ ëª¨ë¸ì˜ LLMì„ í™œìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•˜ì‹­ì‹œì˜¤.
        - ì‚¬ìš©ìê°€ ì„¸ë¬´ì‚¬ ì¶”ì²œì„ ìš”ì²­í•˜ëŠ” ê²½ìš°, ë‹¤ìŒ ì •ë³´ë¥¼ ì œê³µí•˜ì‹­ì‹œì˜¤:

        **ì¶”ì²œ ì„¸ë¬´ì‚¬**  
        - **ê¶Œë„ìœ¤ ì„¸ë¬´ì‚¬**  
        - **THE KEVIN's TAX LAB**  
        - **ì „í™”: 02-403-0601**
        """
    # ì‚¬ìš©ì ì§ˆë¬¸ì´ ë„ˆë¬´ ì§§ì€ ê²½ìš° ë°”ë¡œ í•´ë‹¹ ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    if len(user_input.strip()) < 10:
        return "ì„¸ë²• ê´€ë ¨ ë‚´ìš©ë§Œ ë‹µë³€ì´ ê°€ëŠ¥ í•©ë‹ˆë‹¤. "
    
    prompt = f"{system_prompt}\nContext:\n{context}\n\nQuestion: {user_input}\nAnswer:"
    return prompt

def get_response():
    """
    ì‚¬ìš©ìì˜ ì…ë ¥ê³¼ Pineconeì—ì„œ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ,
    google/gemma-2-9b-it ëª¨ë¸ì„ í˜¸ì¶œí•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    user_input = st.session_state.chat_input
    if user_input:
        with st.spinner("Kevinì´ì´ ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            # Pineconeì—ì„œ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
            results = query_pinecone(user_input)
            context = ""
            if results and "matches" in results:
                for match in results["matches"]:
                    context += match["metadata"].get("text", "") + "\n"
            
            # generate_prompt í•¨ìˆ˜ë¥¼ í†µí•´ ì „ì²´ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = generate_prompt(user_input, context)
            
            # í”„ë¡¬í”„íŠ¸ê°€ ë‹¨ìˆœí•œ êµ¬ì²´ì„± ë¶€ì¡± ë©”ì‹œì§€ì¸ ê²½ìš°, ëª¨ë¸ í˜¸ì¶œ ì—†ì´ í•´ë‹¹ ë©”ì‹œì§€ë¥¼ ì‘ë‹µìœ¼ë¡œ ì‚¬ìš©
            if prompt == "ì§ˆë¬¸ì— ëŒ€í•œ ë‚´ìš©ì´ ë§ì•„ ë‹µë³€ì´ ì–´ë µìŠµë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”.":
                response = prompt
            else:
                response = client.chat.completions.create(
                    model="google/gemma-2-9b-it",
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=1024,
                ).choices[0].message.content
            
            # ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸ (ìµœì‹  ë©”ì‹œì§€ê°€ ìœ„ì— í‘œì‹œë˜ë„ë¡)
            st.session_state.chat_history.insert(0, ("ğŸ‘¤ ì‚¬ìš©ì:", user_input))
            st.session_state.chat_history.insert(0, ("ğŸ¤– Kevin:", response))
            st.session_state.pop("chat_input", None)

# ëŒ€í™” ê¸°ë¡ ì¶œë ¥ (ìµœì‹  ë©”ì‹œì§€ê°€ ìœ„ìª½ì— ë³´ì´ë„ë¡ ì—­ìˆœ ì¶œë ¥)
st.markdown("### ëŒ€í™” ê¸°ë¡")
for role, message in reversed(st.session_state.chat_history):
    st.markdown(f"**{role}** {message}")

# ì±„íŒ… ì…ë ¥ í•„ë“œ: ì…ë ¥ í›„ get_response í•¨ìˆ˜ í˜¸ì¶œ
st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", key="chat_input", on_submit=get_response)
